\documentclass{article}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm}
%\usepackage[square]{natbib}
\usepackage{color}

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{10805 Lab 1}
\author{Subhodeep Moitra \\ {\tt subhodee@andrew.cmu.edu}}
\date{}

\maketitle

\section{Perceptron}

\subsection{Perceptron learning rule}
\begin{enumerate}

\item Weights : 
\begin{verbatim}
--------------------Training Perceptron--------------------
# OR
Final weights are:
[-0.06313892  1.08293573  0.25872327]
--------------------Training Perceptron--------------------
# AND
Final weights are:
[-0.61185395  0.45884488  0.23660592]
--------------------Training Perceptron--------------------
# XOR
###### Not Converged ######
Final weights are:
[ 0.02961735 -0.06937551 -0.35582819]
\end{verbatim}

\item 0 values don't allow change of weights using the perceptron, hence the transformation to -1,1

\end{enumerate}

\subsection{Visualize hypothesis space}
%--


\section{Delta Learning Rule}

\begin{itemize}


\item Weights:
\begin{verbatim}
--------------------Training LMS--------------------
# OR
('Final Prediction is ', array([-1,  1,  1,  1], dtype=int8))
Final weights are:
[-0.49756501  0.99798273  0.99791152]
--------------------Training LMS--------------------
# AND
('Final Prediction is ', array([-1, -1, -1,  1], dtype=int8))
Final weights are:
[-1.49595175  0.99657259  0.99660149]
--------------------Training LMS--------------------
# XOR
('Final Loss is ', array(2.00000001864034))
('Final Prediction is ', array([ 1,  1, -1, -1], dtype=int8))
Final weights are:
[  1.44805136e-04  -1.71599098e-04  -7.25627296e-05]

\end{verbatim}

\end{itemize}

\end{document}
