\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epsfig, graphics}
\usepackage{latexsym}
\usepackage{fullpage}
\usepackage[parfill]{parskip}
%\usepackage{mysymbols}
\usepackage[tight]{subfigure}
\usepackage{hyperref}
\usepackage{amsmath,amssymb,enumerate,comment}
\usepackage{graphicx}

\newcommand{\figref}[1]{Fig.~\ref{#1}}

\title{10725/36725 Optimization \\Homework 1\\
{\small Due September 20, 2012 at beginning of class} \\
{Subhodeep Moitra(subhodee@andrew)}
}
\date{}

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\clearpage

\section{Convexity (Kevin)}

\subsection{Sets}

Let $A\subseteq\mathbb{R}^n$ be a closed set with non-empty interior that has a supporting hyperplane at every point on its boundary.
\begin{enumerate}[(a)]
\item $[$4 pts$]$ 
Show that $A$ is convex.
\end{enumerate}

We will prove this by contradiction. Let us assume that A is not a convex set. Then $\exists$ points $x_1 , x_2 \in A$ s.t. $\theta x_1 + (1-\theta) x_2 \notin A$ for some $\theta \in [0,1]$. Also we know that since A has a supporting hyperplane on every point on its boundary then any hyperplane $a^Tx+b$, on the boundary of A satisfies, $a^Tx_1+b\leq 0$ and $a^Tx_2+b\leq 0$ . But since, $\theta x_1 + (1-\theta) x_2 \notin A$ then there must also exist a hyperplane such that $a^T(\theta x_1 + (1-\theta) x_2)+b > 0$ . We will show that this is impossible. 
\begin{align*}
a^T(\theta x_1 + (1-\theta) x_2)+b > 0 \\
a^T(\theta x_1 + (1-\theta) x_2)+\theta b + (1-\theta)b > 0\\
\theta(a^T x_1+b) + (1-\theta)(a^T x_2+b)  > 0 \\
\end{align*}
But, $ a^Tx_1+b\leq 0$ and $a^Tx_2+b\leq 0$. So, $\theta(a^T x_1+b) + (1-\theta)(a^T x_2+b) \leq 0 $. We arrive a contradiction and hence A is convex.

\vspace{.25cm}

\noindent Let $X,Y\subseteq\mathbb{R}^n$ be disjoint convex sets, let $\{x \mid a^Tx + b = 0\}$ be a separating hyperplane and let $f(x) = Cx + d$ be a function, where $C\in\mathbb{R}^{m\times n},d\in\mathbb{R}^m$.
\begin{enumerate}[(b)]
\item
$[$3 pts$]$ Given that the sets $f(X)$ and $f(Y)$ are disjoint, find a hyperplane, $\{ y \mid \alpha^Ty + \beta = 0 \}$, that separates $f(X)$ and $f(Y)$.
\end{enumerate}

We have that $y = Cx+d$. Let $B \in \mathbb{R}^{n\times m} $ be the left inverse of C. Multiplying the equation on both sides by B, we get. $By = BCx +Bd$ . This simplifies to $x = B(y-d)$ . Hence we have a mapping from y to x. Substituting this mapping into the original hyperplane we obtain

\begin{align*}
a^Tx+b = 0\\
a^TB(y-d)+b = 0 \\
a^TBy - a^TBd + b = 0 \\
\end{align*}

So we obtain that $\alpha = B^Ta$ and $\beta = b-a^TBd$

\subsection{Voronoi Decomposition}

Let $a,b\in\mathbb{R}^n$ such that $a\ne b$.
\begin{enumerate}[(a)]
\item
$[$2 pts$]$  Show that the set $\{ x \mid \Vert x-a \Vert_2 \le \Vert x-b \Vert_2\}$ is a halfspace.
\end{enumerate}

Consider the hyperplane with normal defined by the line joining b and a and passing through the midpoint of a and b. It is represented as $(b-a)^T(x-\frac{a+b}{2})$ . Any point x closer to a than b will make an obtuse angle with $x-\frac{a+b}{2}$ and $b-a$. So, the dot product between these two will be negative. Similarly, any point x closer to b than a will make an acute angle with $x-\frac{a+b}{2}$ and $b-a$ causing the dot product between the two to be positive. We have thus shown that all points which are closer to a than b lie on one side of the hyperplane . This defines a halfspace. 

\vspace{.25cm} 

\noindent Let $x_1,\ldots,x_k\in\mathbb{R}^n$ and let $V_i = \{x \mid \Vert x_i-x \Vert_2 \le \Vert x_j-x \Vert_2, j\ne i \}$.
\begin{enumerate}[(b)]
\item
$[$2 pts$]$  Show that $V_1$ is a polyhedron.  That is, $V_1 = \{x \mid A_1x \le b_1\}$.
\end{enumerate}

Each constraint $\Vert x_i-x \Vert_2 \le \Vert x_j-x \Vert_2$ defines a halfspace. $V_1$ is the intersection of a bunch of halfspaces which defines a polyhedron. 

\vspace{.25cm}

\noindent Let $P_i = \{x \mid A_ix \le b_i\}$ be disjoint polyhedra that cover $\mathbb{R}^n$.
\begin{enumerate}[(c)]
\item
 $[$3 pts$]$  Does there exist points $x_1,\ldots,x_k\in\mathbb{R}^n$ such that $V_i = P_i$ for $i = 1,\ldots,k$?  If so, provide the points, otherwise construct a counterexample.
\end{enumerate}

No, not every polyhedral arrangement needs to give rise to a voronoi diagram. 
\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth]{voronoi}
  \caption{Impossible Voronoi tesselation}
\end{figure}

\clearpage

\subsection{Farkas' Lemma}

Let $A\in\mathbb{R}^{m\times n}, b\in\mathbb{R}^m$ such that there is some $x$ where $Ax = b$.

\begin{enumerate}[(a)]
\item
$[$6 pts$]$  Show that {\bf either} there exists $x > 0$ where $Ax = b$, {\bf or} there exists $\lambda$ such that $A^T\lambda \ge 0$, $A^T\lambda \ne 0$ and $b^T\lambda \le 0$.
\end{enumerate}
Hint: First show that $c^Tx = d$ for all $x$ such that $Ax = b$ if and only if there exists $\lambda$ such that $c= A^T\lambda, d=b^T\lambda$.

The proof of Farkas's lemma is as follows : 
\begin{itemize}
\item We show that it is not possible for both conditions to be satisfied at the same time
\item We show that if the first condition is false then the second condition is true
\end{itemize}


Part 1 : To show that both the conditions cannot be satisfied at the same time. 

Suppose that $\exists x \ge 0$ s.t. $Ax=b$ and $\exists \lambda$ s.t. $A^T\lambda \geq 0$ and $b^T\lambda \leq 0$ . Then by joining both results we have $\lambda^TAx = \lambda^Tb$. This is not possible since $\lambda^TA \geq 0$ and $x \geq 0$ would imply that $\lambda^Tb$ should be non-negative. Hence it is not possible for both conditions to be simultaneously satisfied. 

Part 2 : If the first condition is false then the second condition is true

For this we need the projection theorem. Let K be a convex set and $b \notin K$. Let p is the projection of b on k. i.e. $p = \inf_{x \in K}\Vert x - b \Vert$ . Then for any $x \in K$ we have that $(b-p)^T(x-p) \leq 0$ since the two vectors have an obtuse included angle. 

Now consider that $!\exists x \geq 0$ s.t. $Ax=b$. This means that $b \notin K $ where $K = \{ Ax : x \geq 0\}$. In other words b does not lie in the convex cone of A. We will show that this implies that $\exists \lambda$ s.t. $A^T\lambda \geq 0$ and $b^T\lambda \leq 0$ . 

Let the projection of b on K be $Aw$ where $w \geq 0$ and let $x \geq 0$. Then from the projection them. Let $Aw-b$ be $\lambda$. 
\begin{align*}
(b-Aw)^T(Ax-Aw) \leq 0  \\
-\lambda^TA(x-w) \leq 0 \\
(x-w)^TA^T\lambda \geq 0 \forall x \geq 0 \\
\end{align*}
By choosing $x = w + \textbf{1}_{mx1} $ we can get $A^T\lambda \geq 0$

\begin{equation}
\label{eq1}
\lambda^Tb = \lambda^T(Aw-\lambda) = \lambda^TAw - \lambda^T\lambda
\end{equation}

\begin{align*}
= (b-Aw)^T(Ax-Aw) \leq 0 \text{ [from projection them] }\\
= -\lambda^T(-Aw) \leq 0 \text{ [if $x=0$] } \\
= \lambda^T Aw \leq 0
\end{align*}
and
Since $\lambda =Aw-b$ and $b \notin K$, so $Aw-b \neq 0 => \lambda^T\lambda > 0 => \lambda^T\lambda  < 0$ 

Plugging these two inequalities back into \ref{eq1} we get $\lambda^Tb = \lambda^T(Aw-\lambda) = \lambda^TAw - \lambda^T\lambda < 0$. This concludes part 2. 


\subsection{Functions}

\begin{enumerate}[(a)]
\item
$[$3 pts$]$  Show that the function $f(x) = \left( \sum_{i=1}^n x_i^p\right)^{1/p}$ is concave on $\mathbb{R}^n_{++}$ for all $p\in(0,1)$.
\end{enumerate}
Hint: consider the log-sum-exp and geometric mean functions in Boyd.

\vspace{.5cm}

\begin{enumerate}[(b)]
\item
$[$2 pts$]$  Show that $f(x)$ is convex.
\end{enumerate}

The constraints on y are linear constraints which make the feasible space of y convex. 

\begin{align*}
f(x) = \min_y g(x,y) = \min_y \Vert x - y \Vert 
\end{align*}

$x-y$ is linear in x and y and hence convex. All norms are convex and hence $\Vert x - y \Vert$ is convex. Minimizing along a variable which is convex gives a convex function. 
\newpage
\clearpage


\section{ Alternative formulations (Wooyoung)}

As discussed in class, there are often more than one way to formulate an optimization problem. In this problem, we will go through examples in which you can set up your optimization in a few alternative ways.

\subsection{Rank 1 approximation of matrices }
 
\begin{enumerate}[(a)]
\item 
$[$1.5 pts$]$ How can you reformulate the objective function and the constraint of the above optimization by writing the space of rank 1 matrices as outer products of two vectors?
\end{enumerate}
\vspace{.25cm}

\begin{align*}
\text{minimize} \Vert A-UV^T \Vert^{2}
\end{align*}

where $U \in \mathbb{R}^{m}$ and $V \in \mathbb{R}^{n}$

\begin{enumerate}[(b)]
\item
$[$1.5 pts$]$ One possible disadvantage of writing the space of rank 1 matrices as outer products of two vectors is degeneracy of the solutions; if a pair of vectors $\mathbf{x}$ and $\mathbf{y}$ are solutions, for any non-zero scalar $a$, $a\mathbf{x}$ and $\dfrac{1}{a}\mathbf{y}$ would also be the solutions. How would you reformulate your objective functions and constraints to avoid this degeneracy?
\end{enumerate}
\vspace{.25cm}

\begin{align*}
\text{minimize} \Vert A-\beta UV^T \Vert^{2} \\
\text{s.t.} \Vert U\Vert = 1 \text{ and }\Vert V \Vert =1 
\end{align*}

where $U \in \mathbb{R}^{m}$ and $V \in \mathbb{R}^{n}$


\begin{enumerate}[(c)]
\item 
$[$3 pts$]$ We started with one optimization variable $X$, but now we have more than one. How can you reduce the number of optimization variables in (b)? Show your work. {\it Hint:} try to show that one of the optimization variables is determined when others are fixed.
\end{enumerate}
\vspace{.25cm}

\begin{align*}
\text{minimize} \Vert A- UV^T \Vert^{2} \\
\text{s.t.} \Vert U\Vert = 1 
\end{align*}

where $U \in \mathbb{R}^{m}$ and $V \in \mathbb{R}^{n}$

When the $||U||$ is set to 1 then V is equally adjusted as well. The scaling factor $\beta$ is no longer needed since it is absorbed into V. 

\begin{enumerate}[(d)]
\item 
$[$2 pts$]$ Reformulate the optimization problem in (c) into an equivalent minimization problem with a bi-linear objective function.
\end{enumerate}

\begin{align*}
\text{minimize} \Vert A- UV^T \Vert^{2} \\
\text{s.t.} \Vert U\Vert = 1 
\end{align*}


\vspace{.25cm}

\subsection{Partial minimization of the lasso problem}

%%%%%%%%%%%%%%%
In classical setting of the lasso problem, the sparsity of solutions are enforced by $L1$-norm of the optimization variables ($\mathbf{y} \in \mathcal{R}^{d}$, $A \in \mathcal{R}^{d\times n}$, $\mathbf{x}\in \mathcal{R}^{n}$, $x_{i}$: $i$ th element of $\mathbf{x}$).
\begin{equation}
	\text{min}||\mathbf{y}-A\mathbf{x}||_{2}+\lambda ||\mathbf{x}||_{1}
	\label{lasso}
	\nonumber
\end{equation}
In this problem, we derive the partial minimization of the lasso problem.

\begin{enumerate}[(a)]
\item 
$[$2 pts$]$ How would you reformulate the optimization if we only care about the sparsity of the first $k$ elements of $\mathbf{x}$: $x_{1},x_{2},\cdots,x_{k}$. 
\end{enumerate}

\begin{equation}
	\text{min}||\mathbf{y}-A\mathbf{x}||_{2}^{2}+\lambda \sum_{i=1}^{k} |x_i|
	\label{lasso}
	\nonumber
\end{equation}


\vspace{.25cm}

\begin{enumerate}[(b)]
\item 
$[$3 pts$]$ Show how you can eliminate $x_{k+1},x_{k+2},\cdots,x_{n}$ from the objective function.
\end{enumerate}

Consider A in the block matrix form $A = [A_1 A_2]$ such that $A_1 \in \mathcal{R}^{dxk}$ and $A_2 \in \mathcal{R}^{dx(n-k)}$. Then the optimization problem is 

\begin{equation}
	\text{min}||\mathbf{y}-A_1\mathbf{x_{1:k}}||_{2}^{2}+\lambda \sum_{i=1}^{k} |x_i|
	\label{lasso}
	\nonumber
\end{equation}


\vspace{.25cm}

\subsection{Equality constraint}

\begin{enumerate}[(a)]
\item 
$[$3 pts$]$Can you remove the equality constraint by reformulating $x$ as a linear function of $z$. What conditions does the parameters of your linear function need to satisfy?
\end{enumerate}

\begin{eqnarray*}
&\text{minimize   } f(Fz+x_0)\\
&\text{subject to   } f_i(Fz+x_0) \leq 0, & \text{i=1..m}
\end{eqnarray*}

where $x_0$ denotes any solution to the equality constraints, $F \in \mathbb{R}^{n\times k }$ is a matrix with $\mathcal{R}(F) = \mathcal{N}(A)$ where $z \in \mathbb{R}^k$

\vspace{.25cm}

\subsection{Building a box-shape structure}
You are asked to build a box-shape structure. The area of the wall paper provided to you for the job is $A_{wall}$, and the area of the floor cannot exceed $A_{floor}$. Your picky boss even wants to control the ratio of height ($h$) to width ($w$) and also the ratio of width ($w$) to depth ($d$) of the wall structure; the ratio between $h$ and $w$ should be in the range of [$\alpha$, $\beta$], the ratio between $w$ and $d$ in [$\gamma$,$\delta$]. Your mission is to maximize the volume of the box-shape structure while satisfying all the constraints. 
\begin{enumerate}[(a)]
\item $[$2 pts$]$ Formulate your optimization problem as a maximization problem. Show your objective functions and constraints.
\end{enumerate}

\begin{align*}
\text{ maximize } hdw \\
\text{s.t. } dw <= A_{floor} \\
2(hd +hw) <= A_{wall} \\
\alpha <= h/w <= \beta \\
\gamma \leq w/d \leq \delta \\
w>0 \\
d>0 \\
h>0  \\
\end{align*}
\vspace{.25cm}

\begin{enumerate}[(b)]
\item $[$2 pts$]$ A geometric program is an optimization problem of the form
\begin{eqnarray*}
\text{minimize} & f_{0}(x) &\\
\text{subject to} & f_{i}(x) \leq 1, & i=1,\cdots,m\\
 & g_{i}(x) = 1, & i=1,\cdots,p\\
\end{eqnarray*}
where $g_{i}(x) = c_{i}x_{1}^{a_{1}}x_{2}^{a_{2}}\cdots x_{n}^{a_{n}}, (c_{i}>0, a_{j}\in \mathcal{R}$) and $f_{i}(x) = \sum_{k=1}^{K}c_{i,k}x_{1}^{a_{1k}}x_{2}^{a_{2}}\cdots x_{n}^{a_{nk}}, (c_{i,k}>0, a_{ik}\in \mathcal{R})$. Recently, efficient and robust algorithms have been developed to solve even large-scale geometric programs. Convert the optimization problem in (a) into a geometric programming problem.
\end{enumerate}

\begin{align*}
\text{minimize } h^{-1}d^{-1}w^{-1} \\
\text{s.t. } \frac{1}{A_{floor}}dw \leq 1 \\
\frac{2}{A_{wall}}(hd+hw)  \leq 1 \\
wh^{-1} \leq \alpha \\
hw^{-1} \leq \beta \\
dw^{-1} \leq \gamma \\
wd^{-1} \leq \delta \\
1-h \leq 1 \\
1-d \leq 1 \\
1-w \leq 1
\end{align*}
\vspace{.25cm}

%\begin{enumerate}[(b)]
%\item $[$2 pts$]$ Can you formulate your optimization problem into a minimization problem (not by negating the objective function in (a)) and make all your constraints to be in the form of $f(h,w,d)\leq 1$? Try to minimize the number of constraints and justify your answer.
%\end{enumerate}
%\vspace{.25cm}

\newpage
\clearpage


\section{Node by node (Kevin, Shiva)}


\begin{enumerate}[(a)]
\item $[$10 pts$]$ Derive $-\nabla\ell_T(\Theta)$.
\end{enumerate}

\begin{align*}
-\ell_T(\Theta) = -\textrm{log det}(\Theta) + \textrm{tr}(S\Theta) \\
-\nabla\ell_T(\Theta) = -\nabla \textrm{log det}(\Theta) + \nabla \textrm{tr}(S\Theta) \\
 = -\frac{\nabla \textrm{det}(\Theta)}{\textrm{det}(\Theta)} + \nabla \textrm{tr}(S\Theta)  \\
 = -(\Theta^{-1})^{T} + \nabla \textrm{tr}(S\Theta) 
 = -(\Theta^{-1})^{T} + \nabla \sum_{i=1}^{p} \sum_{k=1}^{p} {s_{ik}\Theta_{ki}} \\
 = -(\Theta^{-1})^{T} + S^T
\end{align*}

\vspace{.25cm}

\begin{enumerate}[(d)]
\item $[$9 pts$]$  Run your algorithm for $K = 5000$ iterations upon $T$ starting from the identity matrix. For each $k \in \{1,\ldots,K\}$, plot the training likelihood $\ell_T(\Theta^{(k)})$ and test likelihood $\ell_U(\Theta^{(k)})$, where $\Theta^{(k)}$ is the $k$th iterate.
\end{enumerate}

\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth]{ll}
  \caption{Log likelihood Plot}
\end{figure}

\begin{enumerate}[(e)]
\item $[$8 pts$]$  Plot the matrix $\Theta^{(K)}$ in a way that emphasizes the sparsity pattern (e.g. \texttt{imagesc}).
\end{enumerate}


\begin{figure}[h!]
  \centering
    \includegraphics[width=\textwidth]{sp}
  \caption{Sparsity Visualization}
\end{figure}


\end{document}
